{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5683431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"G:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m min_max_scaler \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mMinMaxScaler()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\__init__.py:124\u001b[0m\n\u001b[0;32m    122\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    123\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] Файл подкачки слишком мал для завершения операции. Error loading \"G:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\lib\\caffe2_detectron_ops_gpu.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "from binance.spot import Spot \n",
    "from sys import stderr\n",
    "import sys, os,datetime,requests,json,pandas as pd,numpy as np\n",
    "import time,math, gc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70764116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_api(time_last,symbol=\"BTCUSDT\",limit=12*60):\n",
    "    params={\"symbol\":symbol,\"limit\":limit,\"interval\":\"1m\",\"endTime\":time_last,\"startTime\":(time_last-12*3600*1000)}\n",
    "    r=requests.get(url=\"https://api.binance.com/api/v3/klines\", params=params)\n",
    "    df=pd.DataFrame(r.json())\n",
    "    return df\n",
    "\n",
    "def transform_df (df):\n",
    "    column_names=['Open_time','Open','High','Low','Close','Volume','Close_time','Quote_asset_volume',\n",
    "               'Number_of_trades','Taker_buy_base_asset_volume','Taker_buy_quote_asset_volume','Ignore']\n",
    "    df=df.reset_index(drop=True)\n",
    "    df.set_axis(column_names,axis=1,inplace=True)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    weekday=[]\n",
    "    month=[]\n",
    "    for i in range(df['Open'].size):\n",
    "        dt_open=datetime.datetime.fromtimestamp(df['Open_time'][i]//1000)\n",
    "        dt_close=datetime.datetime.fromtimestamp(df['Close_time'][i]//1000)\n",
    "        df['Open_time'][i]=dt_open.hour*3600+dt_open.minute*60+dt_open.second\n",
    "        df['Close_time'][i]=dt_close.hour*3600+dt_close.minute*60+dt_close.second\n",
    "        weekday.append(dt_open.weekday())\n",
    "        month.append(dt_open.month)\n",
    "        #df['Open_time'][i]=dt_open.strftime(\"%I:%M:%S\")\n",
    "        #df['Close_time'][i]=dt_close.strftime(\"%I:%M:%S\")\n",
    "    df['weekday']=weekday\n",
    "    df['month']=month\n",
    "    df=df.drop(df.columns[[7,10,11,13]],axis=1)\n",
    "    del(month)\n",
    "    del(weekday)\n",
    "    return df\n",
    "\n",
    "\n",
    "def moving_average(data,range_,concat=60):\n",
    "    leftover=len(data)%concat\n",
    "    data=data[::concat].reset_index(drop=True)\n",
    "    mean=[]\n",
    "    arr=[]\n",
    "    for i in range(range_):\n",
    "        this_mean=0\n",
    "        for j in range(i+1):\n",
    "            this_mean+=data[j]\n",
    "        mean.append(this_mean/(j+1))\n",
    "    if leftover:\n",
    "        data_size=len(data)-1\n",
    "    else:\n",
    "        data_size=len(data)\n",
    "    for i in range(range_,data_size):\n",
    "        this_mean=data[i]\n",
    "        for j in range(1,range_):\n",
    "            this_mean+=data[i-j]\n",
    "        mean.append(this_mean/(range_))\n",
    "    arr=leftover*[mean[0]]\n",
    "    for i in range(len(mean)):\n",
    "        arr+=concat*[mean[i]]\n",
    "    \n",
    "    \n",
    "    return arr\n",
    "\n",
    "def generate_previous(df,count,col_name):\n",
    "    #idxes=\n",
    "    df=df.reset_index(drop=True)\n",
    "    values=list(df[col_name][0:count])\n",
    "    columns={}\n",
    "    for i in range(count):\n",
    "        this_name=\"prev_\"+col_name+\"_\"+str(i+1)\n",
    "        columns[this_name]=[]\n",
    "    for i in range(count,df[col_name].size):\n",
    "        for j in range(count):\n",
    "            this_name=\"prev_\"+col_name+\"_\"+str(j+1)\n",
    "            columns[this_name].append(values[-(j+1)])\n",
    "        values.pop(0)\n",
    "        values.append(df[col_name][i])\n",
    "    size=df[col_name].size\n",
    "    df=df[count:size]\n",
    "    for i in range(count):\n",
    "        this_name=\"prev_\"+col_name+\"_\"+str(i+1)\n",
    "        df[this_name]=columns[this_name]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def df_to_X_y(df, window_size=5,seq=1):\n",
    "    df_as_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    if (seq):\n",
    "        for i in range(len(df_as_np)-window_size):\n",
    "            row = [a for a in (df.iloc[i:i+window_size].drop(\"target\",axis=1).values)]\n",
    "            X.append(row)\n",
    "            label = df[\"target\"][i+window_size-1]\n",
    "            y.append(label)\n",
    "    else:\n",
    "        for i in range(len(df_as_np)):\n",
    "            row = df.iloc[i].drop(\"target\").values\n",
    "            X.append(row)\n",
    "            label = df[\"target\"][i]\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def tensor_to_X_y(data, window_size=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    data=pd.DataFrame(data.tolist())\n",
    "    columns=data.columns\n",
    "    for i in range(len(data)-window_size):\n",
    "        row = [a for a in (data.iloc[i:i+window_size].drop(data.columns[-1],axis=1).values)]\n",
    "        X.append(row)\n",
    "        label = data[data.columns[-1]][i]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "def preprocess_df(df,target_range,hours,hours_interval,days,days_interval,scaling_range=0.2,scaling=1):\n",
    "    target=[]\n",
    "    mean_price=[]\n",
    "    open_delta=[]\n",
    "    prev_price=[]\n",
    "    concat_hours=3\n",
    "    concat_days=8\n",
    "    window=target_range\n",
    "    #for i in range(df[\"Open\"].size-window):\n",
    "    #    target.append(df[\"Open\"][i+window])\n",
    "    for i in range(df[\"Open\"].size-window):\n",
    "        target.append((df[\"High\"][i+window]+df[\"Low\"][i+window])/2)\n",
    "        mean_price.append((df[\"High\"][i]+df[\"Low\"][i])/2)\n",
    "    df=df[0:df[\"Open\"].size-window]\n",
    "    df[\"mean_price\"]=mean_price\n",
    "    df[\"target\"]=target\n",
    "    df=df.drop([\"Close_time\",\"Taker_buy_base_asset_volume\",\"Volume\",\"Close\"],axis=1)#,\"Low\",\"High\",\"Number_of_trades\"],axis=1)\n",
    "    for i in range(1,hours//hours_interval+1):\n",
    "        df[\"mean_\"+str(i*hours_interval)+\"_hours\"]=moving_average(df[\"Open\"],i*(12//concat_hours)*hours_interval,concat_hours)\n",
    "    for i in range(1,days//days_interval+1):\n",
    "        df[\"mean_\"+str(i*days_interval)+\"_days\"]=moving_average(df[\"Open\"],i*12*(24//concat_days)*days_interval,concat_days)\n",
    "    drop_col=[\"Open\",'Open_time','weekday',\"Number_of_trades\",\"mean_price\"]\n",
    "    df[\"weekday\"]=df[\"weekday\"].astype(float)\n",
    "    df[\"Number_of_trades\"]=df[\"Number_of_trades\"].astype(float)\n",
    "    for i in range(1,df[\"Open\"].size):\n",
    "        df[\"Open_time\"][i]/=86400\n",
    "        df[\"weekday\"][i]/=6.0\n",
    "        df[\"Number_of_trades\"][i]/=10000\n",
    "        delta=df[\"mean_price\"][i]-df[\"mean_price\"][i-1]\n",
    "        prev_price.append(df[\"mean_price\"][i-1])\n",
    "        open_delta.append(delta)\n",
    "    df=df.drop(0,axis=0)\n",
    "    #df[\"mean_price_delta\"]=open_delta\n",
    "    df[\"prev_price\"]=prev_price\n",
    "    for col in df.drop(drop_col,axis=1).columns:\n",
    "        for i in range(1,df[\"Open\"].size+1):\n",
    "            #df[col][i]=df[col][i]/df[\"Open\"][i]\n",
    "            df[col][i]=df[col][i]/df[\"mean_price\"][i]\n",
    "            pass\n",
    "    cols=df.drop(drop_col,axis=1).columns\n",
    "    if scaling:\n",
    "        for i in cols:\n",
    "            for j in range(1,df[\"Open\"].size+1):\n",
    "                df[i][j]=(df[i][j]-(1-scaling_range))/(scaling_range*2)\n",
    "    return df\n",
    "\n",
    "def upscale(input_data,scaling_range):\n",
    "    return input_data*2*scaling_range-scaling_range+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe66fda3-b348-4409-a07d-5020ce74bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_heatmap(df):\n",
    "    corr=df.corr()\n",
    "    result=sns.heatmap(corr, \n",
    "            xticklabels=corr.columns,\n",
    "            yticklabels=corr.columns)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad7e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d932e0eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mautoencoder\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,drop,hidden_size,test_size):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(autoencoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,drop,hidden_size,test_size):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.norm=nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.Linear(41,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_size, test_size,bias=True)\n",
    "        self.fc4 = nn.Linear(test_size, hidden_size,bias=True)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc7 = nn.Linear(hidden_size, 41,bias=True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "    def encode (self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16170cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70430d44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#RELUS\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet_r\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,drop,hidden_size,input_size):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Net_r, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#RELUS\n",
    "\n",
    "class Net_r(nn.Module):\n",
    "    def __init__(self,drop,hidden_size,input_size):\n",
    "        super(Net_r, self).__init__()\n",
    "        self.lstm_active=0\n",
    "        self.norm=nn.BatchNorm1d(hidden_size)\n",
    "        self.lstm = nn.LSTM(num_layers=1,input_size=input_size, hidden_size=hidden_size,batch_first=True)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size,bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc6 = nn.Linear(hidden_size, 1,bias=True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self, x,train):\n",
    "        if (self.lstm_active):\n",
    "            x,_ = self.lstm(x)\n",
    "            x=x[:,-1,:]\n",
    "        else:\n",
    "            x=self.fc1(x)\n",
    "        #x=self.norm(x)\n",
    "        x=F.relu(x)\n",
    "        if train:\n",
    "            #x=self.norm(x)\n",
    "            pass\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=self.fc3(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        if train:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x=self.fc4(x)\n",
    "        x=F.tanh(x)\n",
    "        \n",
    "        x=self.fc5(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        if train:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787c84a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#LSTM##\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,drop,hidden_size,input_size):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Net, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#LSTM##\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,drop,hidden_size,input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm_active=0\n",
    "        #self.norm=nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.LSTM(num_layers=1,input_size=input_size, hidden_size=hidden_size,batch_first=True)#lstm input\n",
    "        self.fc2 = nn.Linear(input_size, hidden_size//2,bias=True)#standart input\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc5 = nn.Linear(hidden_size//2, hidden_size//4,bias=True)\n",
    "        self.fc6 = nn.Linear(hidden_size//4, 1,bias=True)\n",
    "        #self.dropout = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        if (self.lstm_active):\n",
    "            x,_ = self.fc1(x)\n",
    "            x=x[:,-1,:]\n",
    "        else:\n",
    "            x=self.fc2(x)\n",
    "        \n",
    "        #x=F.tanh(x)\n",
    "        #x = self.fc3(x)\n",
    "        x=F.tanh(x)\n",
    "        #x = F.logsigmoid(x)\n",
    "        #x=F.relu(x)\n",
    "        #x = F.logsigmoid(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be953dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_func(pred_data,real_data):\n",
    "    mae=0\n",
    "    for i in range(len(pred_data)):\n",
    "        mae+=abs(pred_data[i]-real_data[i])\n",
    "    mae/=len(pred_data)\n",
    "    return mae\n",
    "\n",
    "\n",
    "def train(learning_rate,\n",
    "          batch_size,epochs,\n",
    "          momentum,train_data,test_data,X_train_opens,X_test_opens,\n",
    "          net,net_max_std,net_min_err_train,net_min_err_test,\n",
    "          decay=0,min_error=0,direction_punish=0,direction_reward=0,scaling=1,directory=\"streamlit_data\"):\n",
    "    loss_train_single=[]\n",
    "    loss_test_single=[]\n",
    "    stds_single=[]\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    scaling_range=0.2\n",
    "    X_train=train_data[0]\n",
    "    X_test=test_data[0]\n",
    "    y_train=train_data[1]\n",
    "    y_test=test_data[1]\n",
    "    \n",
    "    if (train_data[0].shape[0]>2000):\n",
    "        parting=X_train.shape[0]//500\n",
    "        X_test_part=X_test[::parting]\n",
    "        X_train_part=X_train[::parting*10]\n",
    "        y_test_part=y_test[::parting]\n",
    "        y_train_part=y_train[::parting*10]\n",
    "        X_train_part_opens=X_train_opens[::parting*10]\n",
    "        X_test_part_opens=X_test_opens[::parting]\n",
    "    else:\n",
    "        parting=X_train.shape[0]//1#//500\n",
    "        X_test_part=X_test#[::parting]\n",
    "        X_train_part=X_train#[::parting*10]\n",
    "        y_test_part=y_test#[::parting]\n",
    "        y_train_part=y_train#[::parting*10]\n",
    "        X_train_part_opens=X_train_opens\n",
    "        X_test_part_opens=X_test_opens\n",
    "    train_batches=len(X_train)//batch_size\n",
    "    last_batch=len(X_train)%batch_size+1\n",
    "    #net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum,weight_decay=decay)\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,weight_decay=decay)\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    train_loss=nn.L1Loss()\n",
    "    loss_train=[]\n",
    "    loss_test=[]\n",
    "    y_test_part_scaled=torch.tensor(y_test_part,device=\"cuda\")\n",
    "    y_train_part_scaled=torch.tensor(y_train_part,device=\"cuda\")\n",
    "    \n",
    "    if scaling:\n",
    "        y_test_part_scaled=upscale(y_test_part_scaled,scaling_range)*X_test_part_opens\n",
    "        y_train_part_scaled=upscale(y_train_part_scaled,scaling_range)*X_train_part_opens\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(train_batches+1):\n",
    "            if (batch<train_batches):\n",
    "                X_train_batch=torch.tensor(X_train[batch*batch_size:(batch_size*(batch+1))],device=\"cuda\")\n",
    "                X_opens_batch=X_train_opens[batch*batch_size:(batch_size*(batch+1))]\n",
    "                y_train_batch=torch.tensor(y_train[batch*batch_size:(batch_size*(batch+1))],device=\"cuda\")\n",
    "            else:\n",
    "                X_train_batch=torch.tensor(X_train[batch*batch_size:batch*batch_size+last_batch],device=\"cuda\")\n",
    "                X_opens_batch=X_train_opens[batch*batch_size:batch*batch_size+last_batch]\n",
    "                y_train_batch=torch.tensor(y_train[batch*batch_size:batch*batch_size+last_batch],device=\"cuda\")\n",
    "            #y_targets=upscale(y_train_batch,scaling_range)*X_opens_batch\n",
    "            net_out=net(X_train_batch).reshape(1,-1)[0]#*X_opens_batch\n",
    "            #net_out=net.predict(X_train_batch).reshape(1,-1)[0]#*X_opens_batch\n",
    "            loss = criterion(net_out,y_train_batch)#,y_targets)\n",
    "            #loss.requires_grad=True\n",
    "            #loss = torch.sqrt(criterion(net_out, torch.tensor(y_train_batch,device=\"cuda\")))\n",
    "            if scaling:\n",
    "                for i in range(net_out.shape[0]):\n",
    "                    if((net_out[i]-0.5)*(y_train_batch[i]-0.5)<0):\n",
    "                        loss*=(1+direction_punish)\n",
    "                    else:\n",
    "                        loss*=(1-direction_reward)\n",
    "            #optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #with torch.no_grad():\n",
    "        \n",
    "        #net_out_test=net(torch.tensor(X_test_part,device=\"cuda\"))\n",
    "        #net_out_test=net.predict(torch.tensor(X_test_part,device=\"cuda\")).detach().reshape(1,-1)[0]\n",
    "        #net_out_train=net.predict(torch.tensor(X_train_part,device=\"cuda\")).detach().reshape(1,-1)[0]\n",
    "        net_out_test=net(torch.tensor(X_test_part,device=\"cuda\").detach()).reshape(1,-1)[0]\n",
    "        net_out_train=net(torch.tensor(X_train_part,device=\"cuda\").detach()).reshape(1,-1)[0]\n",
    "        if scaling:\n",
    "            net_out_train=upscale(net_out_train,scaling_range)*X_train_part_opens\n",
    "            net_out_test=upscale(net_out_test,scaling_range)*X_test_part_opens\n",
    "            #print(net_out_test[:20])\n",
    "            #mae=mae_func(net_out_test,y_test_part_scaled)\n",
    "            #loss_test.append(train_loss(net_out_test,y_test_part_scaled))#.item())\n",
    "            loss_test_single.append(train_loss(net_out_test,torch.tensor(y_test_part_scaled,device=\"cuda\")).item())#(mae.item())\n",
    "            #net_out_train=net(torch.tensor(X_train_part,device=\"cuda\"))\n",
    "            #loss_train.append(train_loss(net_out_train,y_train_part_scaled))#.item())\n",
    "            loss_train_single.append(train_loss(net_out_train,torch.tensor(y_train_part_scaled,device=\"cuda\")).item())\n",
    "        else:\n",
    "            #loss_test.append(train_loss(net_out_test,y_test_part))#.item())\n",
    "            loss_test_single.append(train_loss(net_out_test,torch.tensor(y_test_part,device=\"cuda\")).item())#(mae.item())\n",
    "            #net_out_train=net(torch.tensor(X_train_part,device=\"cuda\"))\n",
    "            #loss_train.append(train_loss(net_out_train,y_train_part))#.item())\n",
    "            loss_train_single.append(train_loss(net_out_train,torch.tensor(y_train_part,device=\"cuda\")).item())\n",
    "            \n",
    "            gc.collect()\n",
    "        #this_std=np.std(net.predict(torch.tensor(X_test[::70],device=\"cuda\") ).tolist())\n",
    "        #this_std=np.std(net.predict(torch.tensor(X_test,device=\"cuda\") ).tolist())\n",
    "        this_std=np.std(net(torch.tensor(X_test,device=\"cuda\") ).tolist())\n",
    "        stds_single.append(this_std)\n",
    "        if (epoch==0):\n",
    "            max_std=0\n",
    "            min_err_test=loss_test_single[-1]\n",
    "            min_err_train=loss_train_single[-1]\n",
    "            \n",
    "            \n",
    "        if (this_std>max_std):\n",
    "            path_std=os.path.join(directory,\"std_net.pth\")\n",
    "            torch.save(net.state_dict(), path_std)\n",
    "            net_max_std.load_state_dict(torch.load(path_std))\n",
    "            max_std=this_std\n",
    "        if (loss_train_single[-1]<min_err_train):\n",
    "            path_min_err_train=os.path.join(directory,\"err_train_net.pth\")\n",
    "            torch.save(net.state_dict(), path_min_err_train)\n",
    "            net_min_err_train.load_state_dict(torch.load(path_min_err_train))\n",
    "            min_err_train=loss_train_single[-1]\n",
    "        if (loss_test_single[-1]<min_err_test):\n",
    "            path_min_err_test=os.path.join(directory,\"err_test_net.pth\")\n",
    "            torch.save(net.state_dict(), path_min_err_test)\n",
    "            net_min_err_test.load_state_dict(torch.load(path_min_err_test))\n",
    "            min_err_test=loss_test_single[-1]\n",
    "        if (epoch%5==1):\n",
    "            #print(\"train_loss: \"+str(loss_train_single[-1])+ \" / test loss: \"+str(loss_test_single[-1])+\" / std: \"+str(this_std))\n",
    "            print('train_loss: %.5f / test loss: %.5f / std: %.5f'% (loss_train_single[-1],loss_test_single[-1], this_std) )\n",
    "        #if (loss_test_single[-1]<min_error):\n",
    "        if (loss_train_single[-1]<min_error):\n",
    "            break\n",
    "        \n",
    "    return loss_train_single,loss_test_single,stds_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d94339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_train(file,encoder=0,lstm=0, window_size=30,scaling=1):\n",
    "    scaling_range=0.2\n",
    "    df=pd.read_csv(file).reset_index(drop=True)\n",
    "    df=preprocess_df(df,6,4,1,21,4,scaling=scaling,scaling_range=scaling_range).reset_index(drop=True)\n",
    "    if (encoder==0 and lstm):\n",
    "        X,y=df_to_X_y(df,window_size) #for no enc\n",
    "    if (encoder==0 and lstm==0):\n",
    "        X,y=df_to_X_y(df,window_size,0) #no lstm no enc\n",
    "    if encoder:\n",
    "        X,y=tensor_to_X_y(arr,window_size) #for enc\n",
    "        encoder=autoencoder(0,64,10).cuda().double()\n",
    "        encoder.load_state_dict(torch.load(directory+'\\\\encoder_05-07-2022_22-03-48_64_10.pth'))\n",
    "        encoder.eval()\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                            test_size = 0.10,\n",
    "                                                            random_state =123,\n",
    "                                                            shuffle=False)\n",
    "\n",
    "    train_idxes=sklearn.utils.shuffle(range(len(X_train)))\n",
    "    X_train_shuffled=[]\n",
    "    y_train_shuffled=[]\n",
    "    for i in train_idxes:\n",
    "        X_train_shuffled.append(X_train[i])\n",
    "        y_train_shuffled.append(y_train[i])\n",
    "    X_train=np.array(X_train_shuffled[:500])\n",
    "    y_train=np.array(y_train_shuffled[:500])\n",
    "    del([X_train_shuffled,y_train_shuffled,train_idxes])\n",
    "\n",
    "\n",
    "    if (encoder==0 and lstm==0):\n",
    "        X_train_opens=torch.tensor(X_train[:,1],device=\"cuda\") #for df_to_x_y\n",
    "        X_test_opens=torch.tensor(X_test[:,1],device=\"cuda\")\n",
    "        if scaling:\n",
    "            X_train=np.concatenate([X_train[:,2:],X_train[:,[0]]],axis=1)\n",
    "            X_test=np.concatenate([X_test[:,2:],X_test[:,[0]]],axis=1)\n",
    "\n",
    "    if (encoder==0 and lstm==1):\n",
    "        X_train_opens=torch.tensor(X_train[:,:,1][:,window_size-1],device=\"cuda\") #for df_to_x_y, lstm\n",
    "        X_test_opens=torch.tensor(X_test[:,:,1][:,window_size-1],device=\"cuda\")\n",
    "        if scaling:\n",
    "            X_train=np.concatenate([X_train[:,:,2:],X_train[:,:,[0]]],axis=2)\n",
    "            X_test=np.concatenate([X_test[:,:,2:],X_test[:,:,[0]]],axis=2)\n",
    "\n",
    "    if(encoder==1):\n",
    "        X_train_opens=torch.tensor(X_train[:,:,-1][:,window_size-1],device=\"cuda\") #for tensor_to_x_y\n",
    "        X_test_opens=torch.tensor(X_test[:,:,-1][:,window_size-1],device=\"cuda\")\n",
    "        if scaling:\n",
    "            X_train=X_train[:,:,:-1]\n",
    "            X_test=X_test[:,:,:-1]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_opens, X_test_opens\n",
    "    #LOAD NET\n",
    "    #name=\"\\\\net_24-10-2022_18-33-11_16_128\"\n",
    "    #path=directory+name\n",
    "    #net=Net(0.6,128,16).cuda().double()\n",
    "    #net.load_state_dict(torch.load(path+\".pth\"))\n",
    "    #net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ed0e74b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:33\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def train_bot(X_train, X_test, y_train, y_test,X_train_opens,X_test_opens,epochs=1000,lr=0.001,hidden_size=512,drop=0,batch_sz=25,normalized_input=0,decay=0,momentum=0.8,grid_search=0,scaling=1):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    torch.cuda.empty_cache() \n",
    "    X_train_normalized=torch.nn.functional.normalize(torch.tensor(X_train,device=\"cuda\"),2,0)\n",
    "    X_test_normalized=torch.nn.functional.normalize(torch.tensor(X_test,device=\"cuda\"),2,0)\n",
    "    net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    runs_count=0\n",
    "    net_max_std=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    net_min_err_train=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    net_min_err_test=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    \n",
    "    min_err=0.0007\n",
    "    runs_count+=1\n",
    "\n",
    "   \n",
    "    if (grid_search):\n",
    "        min_loss_train_arr=[]\n",
    "        min_loss_test_arr=[]\n",
    "        for hidden_size_idx in range(1,10):\n",
    "            hidden_size=hidden_size_idx*64\n",
    "            net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "            net_max_std=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "            net_min_err_train=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "            net_min_err_test=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "            for batch_sz_idx in range(1,2):\n",
    "                for lr_idx in range (1,4):\n",
    "                    loss_train_single,loss_test_single,stds_single=train(net=net,net_max_std=net_max_std,net_min_err_train=net_min_err_train,net_min_err_test=net_min_err_test,\n",
    "                                                                         learning_rate=0.05/(10*lr_idx), \n",
    "                                                                         batch_size=50-batch_sz_idx*10, epochs=1000, momentum=momentum,\n",
    "                                                                         decay=decay,train_data=[X_train,y_train],test_data=[X_test,y_test],\n",
    "                                                                         X_train_opens=X_train_opens,X_test_opens=X_test_opens,min_error=min_err)\n",
    "            min_loss_train_arr.append(np.asarray(net_min_err_train(torch.tensor(X_test,device=\"cuda\")).tolist()))\n",
    "            min_loss_test_arr.append(np.asarray(net_min_err_test(torch.tensor(X_test,device=\"cuda\")).tolist()))   \n",
    "    else:\n",
    "        if normalized_input:\n",
    "            loss_train_single,loss_test_single,stds_single=train(net=net,net_max_std=net_max_std,net_min_err_train=net_min_err_train,net_min_err_test=net_min_err_test,\n",
    "                  learning_rate=lr, batch_size=batch_sz, epochs=epochs, momentum=momentum,decay=decay,\n",
    "                  train_data=[X_train_normalized,y_train],test_data=[X_test_normalized,y_test],X_train_opens=X_train_opens,X_test_opens=X_test_opens,min_error=min_err)#,direction_punish=0.1,direction_reward=0.1)\n",
    "        else:\n",
    "            loss_train_single,loss_test_single,stds_single=train(net=net,net_max_std=net_max_std,net_min_err_train=net_min_err_train,net_min_err_test=net_min_err_test,\n",
    "                  learning_rate=lr, batch_size=batch_sz, epochs=epochs, momentum=momentum,decay=decay,\n",
    "                  train_data=[X_train,y_train],test_data=[X_test,y_test],X_train_opens=X_train_opens,X_test_opens=X_test_opens,min_error=min_err)\n",
    "        err_df=pd.DataFrame({\"train\":loss_train_single,\"test\":loss_test_single})\n",
    "        return net,net_min_err_train,net_min_err_test,net_max_std,err_df,stds_single\n",
    "    return net,net_min_err_train,net_min_err_test,net_max_std,min_loss_train_arr,min_loss_test_arr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124be9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c0c10a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_viz_1():\n",
    "    if normalized_input:\n",
    "        preds=pd.DataFrame(net(torch.tensor(X_test_normalized,device=\"cuda\")).tolist())\n",
    "        array=np.asarray(net_max_std(torch.tensor(X_test_normalized,device=\"cuda\")).tolist())\n",
    "        min_train=np.asarray(net_min_err_train(torch.tensor(X_train_normalized,device=\"cuda\")).tolist())\n",
    "    else:\n",
    "        preds=pd.DataFrame(net(torch.tensor(X_test,device=\"cuda\")).tolist())\n",
    "        array=np.asarray(net_max_std(torch.tensor(X_test,device=\"cuda\")).tolist())\n",
    "        min_train=np.asarray(net_min_err_train(torch.tensor(X_test,device=\"cuda\")).tolist())\n",
    "\n",
    "    #data=pd.DataFrame(y_test[::70][:-15])\n",
    "    data=pd.DataFrame({\"targets\":y_test})\n",
    "    data[\"preds\"]=preds\n",
    "    data[\"max_std_preds\"]=array\n",
    "    data[\"min_train\"]=min_train\n",
    "    #data[\"0.5\"]=pd.DataFrame([0.5 for i in range(preds.size)])\n",
    "    data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e5f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_viz_2():\n",
    "    if X_train.shape[0]>1000:\n",
    "        if normalized_input:\n",
    "            preds=pd.DataFrame(net(torch.tensor(X_train_normalized[::100][-15:],device=\"cuda\")).tolist())\n",
    "            array=np.asarray(net_max_std(torch.tensor(X_train_normalized[::100][-15:],device=\"cuda\")).tolist())\n",
    "            min_train=np.asarray(net_min_err_train(torch.tensor(X_train_normalized[::100][-15:],device=\"cuda\")).tolist())\n",
    "        else:\n",
    "            preds=pd.DataFrame(net(torch.tensor(X_train[::100][-15:],device=\"cuda\")).tolist())\n",
    "            array=np.asarray(net_max_std(torch.tensor(X_train[::100][-15:],device=\"cuda\")).tolist())\n",
    "            min_train=np.asarray(net_min_err_train(torch.tensor(X_train[::100][-15:],device=\"cuda\")).tolist())\n",
    "        data=pd.DataFrame({\"targets\":y_train[::100][-15:]})\n",
    "    else:\n",
    "        if normalized_input:\n",
    "            preds=pd.DataFrame(net(torch.tensor(X_train_normalized,device=\"cuda\")).tolist())\n",
    "            array=np.asarray(net_max_std(torch.tensor(X_train_normalized,device=\"cuda\")).tolist())\n",
    "            min_train=np.asarray(net_min_err_train(torch.tensor(X_train_normalized,device=\"cuda\")).tolist())\n",
    "        else:\n",
    "            preds=pd.DataFrame(net(torch.tensor(X_train,device=\"cuda\")).tolist())\n",
    "            array=np.asarray(net_max_std(torch.tensor(X_train,device=\"cuda\")).tolist())\n",
    "            min_train=np.asarray(net_min_err_train(torch.tensor(X_train,device=\"cuda\")).tolist())\n",
    "        data=pd.DataFrame({\"targets\":y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f268ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_net(net,path):\n",
    "    torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3917e3b-bcb2-4076-ba93-9772b6a2ef11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
