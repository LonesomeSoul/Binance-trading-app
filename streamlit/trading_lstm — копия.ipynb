{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5683431",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     16\u001b[0m min_max_scaler \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mMinMaxScaler()\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from binance.spot import Spot \n",
    "from sys import stderr\n",
    "import sys, os,datetime,requests,json,pandas as pd,numpy as np\n",
    "import time,math, gc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70764116",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def import_api(time_last,symbol=\"BTCUSDT\",limit=12*60):\n",
    "    params={\"symbol\":symbol,\"limit\":limit,\"interval\":\"1m\",\"endTime\":time_last,\"startTime\":(time_last-12*3600*1000)}\n",
    "    r=requests.get(url=\"https://api.binance.com/api/v3/klines\", params=params)\n",
    "    df=pd.DataFrame(r.json())\n",
    "    return df\n",
    "\n",
    "def transform_df (df):\n",
    "    column_names=['Open_time','Open','High','Low','Close','Volume','Close_time','Quote_asset_volume',\n",
    "               'Number_of_trades','Taker_buy_base_asset_volume','Taker_buy_quote_asset_volume','Ignore']\n",
    "    df=df.reset_index(drop=True)\n",
    "    df.set_axis(column_names,axis=1,inplace=True)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype(float)\n",
    "    weekday=[]\n",
    "    month=[]\n",
    "    for i in range(df['Open'].size):\n",
    "        dt_open=datetime.datetime.fromtimestamp(df['Open_time'][i]//1000)\n",
    "        dt_close=datetime.datetime.fromtimestamp(df['Close_time'][i]//1000)\n",
    "        df['Open_time'][i]=dt_open.hour*3600+dt_open.minute*60+dt_open.second\n",
    "        df['Close_time'][i]=dt_close.hour*3600+dt_close.minute*60+dt_close.second\n",
    "        weekday.append(dt_open.weekday())\n",
    "        month.append(dt_open.month)\n",
    "        #df['Open_time'][i]=dt_open.strftime(\"%I:%M:%S\")\n",
    "        #df['Close_time'][i]=dt_close.strftime(\"%I:%M:%S\")\n",
    "    df['weekday']=weekday\n",
    "    df['month']=month\n",
    "    df=df.drop(df.columns[[7,10,11,13]],axis=1)\n",
    "    del(month)\n",
    "    del(weekday)\n",
    "    return df\n",
    "\n",
    "\n",
    "def moving_average(data,range_,concat=60):\n",
    "    leftover=len(data)%concat\n",
    "    data=data[::concat].reset_index(drop=True)\n",
    "    mean=[]\n",
    "    arr=[]\n",
    "    for i in range(range_):\n",
    "        this_mean=0\n",
    "        for j in range(i+1):\n",
    "            this_mean+=data[j]\n",
    "        mean.append(this_mean/(j+1))\n",
    "    if leftover:\n",
    "        data_size=len(data)-1\n",
    "    else:\n",
    "        data_size=len(data)\n",
    "    for i in range(range_,data_size):\n",
    "        this_mean=data[i]\n",
    "        for j in range(1,range_):\n",
    "            this_mean+=data[i-j]\n",
    "        mean.append(this_mean/(range_))\n",
    "    arr=leftover*[mean[0]]\n",
    "    for i in range(len(mean)):\n",
    "        arr+=concat*[mean[i]]\n",
    "    \n",
    "    \n",
    "    return arr\n",
    "\n",
    "def generate_previous(df,count,col_name):\n",
    "    #idxes=\n",
    "    df=df.reset_index(drop=True)\n",
    "    values=list(df[col_name][0:count])\n",
    "    columns={}\n",
    "    for i in range(count):\n",
    "        this_name=\"prev_\"+col_name+\"_\"+str(i+1)\n",
    "        columns[this_name]=[]\n",
    "    for i in range(count,df[col_name].size):\n",
    "        for j in range(count):\n",
    "            this_name=\"prev_\"+col_name+\"_\"+str(j+1)\n",
    "            columns[this_name].append(values[-(j+1)])\n",
    "        values.pop(0)\n",
    "        values.append(df[col_name][i])\n",
    "    size=df[col_name].size\n",
    "    df=df[count:size]\n",
    "    for i in range(count):\n",
    "        this_name=\"prev_\"+col_name+\"_\"+str(i+1)\n",
    "        df[this_name]=columns[this_name]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def df_to_X_y(df, window_size=5,seq=1):\n",
    "    df_as_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    if (seq):\n",
    "        for i in range(len(df_as_np)-window_size):\n",
    "            row = [a for a in (df.iloc[i:i+window_size].drop(\"target\",axis=1).values)]\n",
    "            X.append(row)\n",
    "            label = df[\"target\"][i+window_size-1]\n",
    "            y.append(label)\n",
    "    else:\n",
    "        for i in range(len(df_as_np)):\n",
    "            row = df.iloc[i].drop(\"target\").values\n",
    "            X.append(row)\n",
    "            label = df[\"target\"][i]\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def tensor_to_X_y(data, window_size=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    data=pd.DataFrame(data.tolist())\n",
    "    columns=data.columns\n",
    "    for i in range(len(data)-window_size):\n",
    "        row = [a for a in (data.iloc[i:i+window_size].drop(data.columns[-1],axis=1).values)]\n",
    "        X.append(row)\n",
    "        label = data[data.columns[-1]][i]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "def preprocess_df(df,target_range,hours,hours_interval,days,days_interval,scaling_range=0.2,scaling=1):\n",
    "    target=[]\n",
    "    mean_price=[]\n",
    "    open_delta=[]\n",
    "    prev_price=[]\n",
    "    concat_hours=3\n",
    "    concat_days=8\n",
    "    window=target_range\n",
    "    #for i in range(df[\"Open\"].size-window):\n",
    "    #    target.append(df[\"Open\"][i+window])\n",
    "    for i in range(df[\"Open\"].size-window):\n",
    "        target.append((df[\"High\"][i+window]+df[\"Low\"][i+window])/2)\n",
    "        mean_price.append((df[\"High\"][i]+df[\"Low\"][i])/2)\n",
    "    df=df[0:df[\"Open\"].size-window]\n",
    "    df[\"mean_price\"]=mean_price\n",
    "    df[\"target\"]=target\n",
    "    df=df.drop([\"Close_time\",\"Taker_buy_base_asset_volume\",\"Volume\",\"Close\"],axis=1)#,\"Low\",\"High\",\"Number_of_trades\"],axis=1)\n",
    "    for i in range(1,hours//hours_interval+1):\n",
    "        df[\"mean_\"+str(i*hours_interval)+\"_hours\"]=moving_average(df[\"Open\"],i*(12//concat_hours)*hours_interval,concat_hours)\n",
    "    for i in range(1,days//days_interval+1):\n",
    "        df[\"mean_\"+str(i*days_interval)+\"_days\"]=moving_average(df[\"Open\"],i*12*(24//concat_days)*days_interval,concat_days)\n",
    "    drop_col=[\"Open\",'Open_time','weekday',\"Number_of_trades\",\"mean_price\"]\n",
    "    df[\"weekday\"]=df[\"weekday\"].astype(float)\n",
    "    df[\"Number_of_trades\"]=df[\"Number_of_trades\"].astype(float)\n",
    "    for i in range(1,df[\"Open\"].size):\n",
    "        df[\"Open_time\"][i]/=86400\n",
    "        df[\"weekday\"][i]/=6.0\n",
    "        df[\"Number_of_trades\"][i]/=10000\n",
    "        delta=df[\"mean_price\"][i]-df[\"mean_price\"][i-1]\n",
    "        prev_price.append(df[\"mean_price\"][i-1])\n",
    "        open_delta.append(delta)\n",
    "    df=df.drop(0,axis=0)\n",
    "    #df[\"mean_price_delta\"]=open_delta\n",
    "    df[\"prev_price\"]=prev_price\n",
    "    for col in df.drop(drop_col,axis=1).columns:\n",
    "        for i in range(1,df[\"Open\"].size+1):\n",
    "            #df[col][i]=df[col][i]/df[\"Open\"][i]\n",
    "            df[col][i]=df[col][i]/df[\"mean_price\"][i]\n",
    "            pass\n",
    "    cols=df.drop(drop_col,axis=1).columns\n",
    "    if scaling:\n",
    "        for i in cols:\n",
    "            for j in range(1,df[\"Open\"].size+1):\n",
    "                df[i][j]=(df[i][j]-(1-scaling_range))/(scaling_range*2)\n",
    "    return df\n",
    "\n",
    "def upscale(input_data,scaling_range):\n",
    "    return input_data*2*scaling_range-scaling_range+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b0fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"G:\\Учеба\\биржа\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f813a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danielbordach\\AppData\\Local\\Temp\\ipykernel_13156\\1351517506.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"mean_price\"]=mean_price\n",
      "C:\\Users\\danielbordach\\AppData\\Local\\Temp\\ipykernel_13156\\1351517506.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"target\"]=target\n"
     ]
    }
   ],
   "source": [
    "scaling=1\n",
    "scaling_range=0.2\n",
    "df=pd.read_csv(directory+'\\\\eth_usdt_5min.csv')[20000:220000].reset_index(drop=True)\n",
    "df=preprocess_df(df,6,4,1,21,4,scaling=scaling,scaling_range=scaling_range).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0041b517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Number_of_trades</th>\n",
       "      <th>weekday</th>\n",
       "      <th>mean_price</th>\n",
       "      <th>target</th>\n",
       "      <th>mean_1_hours</th>\n",
       "      <th>mean_2_hours</th>\n",
       "      <th>mean_3_hours</th>\n",
       "      <th>mean_4_hours</th>\n",
       "      <th>mean_4_days</th>\n",
       "      <th>mean_8_days</th>\n",
       "      <th>mean_12_days</th>\n",
       "      <th>mean_16_days</th>\n",
       "      <th>mean_20_days</th>\n",
       "      <th>prev_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927083</td>\n",
       "      <td>1948.28</td>\n",
       "      <td>0.514096</td>\n",
       "      <td>0.485904</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1958.835</td>\n",
       "      <td>0.483083</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.503644</td>\n",
       "      <td>0.491685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.930556</td>\n",
       "      <td>1966.87</td>\n",
       "      <td>0.506678</td>\n",
       "      <td>0.493322</td>\n",
       "      <td>0.3711</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1963.545</td>\n",
       "      <td>0.471149</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.497638</td>\n",
       "      <td>0.494003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.934028</td>\n",
       "      <td>1958.46</td>\n",
       "      <td>0.510884</td>\n",
       "      <td>0.489116</td>\n",
       "      <td>0.6934</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1953.495</td>\n",
       "      <td>0.486006</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.510488</td>\n",
       "      <td>0.512862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>1946.10</td>\n",
       "      <td>0.506371</td>\n",
       "      <td>0.493629</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1950.290</td>\n",
       "      <td>0.484913</td>\n",
       "      <td>0.512543</td>\n",
       "      <td>0.512543</td>\n",
       "      <td>0.512543</td>\n",
       "      <td>0.512543</td>\n",
       "      <td>0.514613</td>\n",
       "      <td>0.514613</td>\n",
       "      <td>0.514613</td>\n",
       "      <td>0.514613</td>\n",
       "      <td>0.514613</td>\n",
       "      <td>0.504108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940972</td>\n",
       "      <td>1948.30</td>\n",
       "      <td>0.503351</td>\n",
       "      <td>0.496649</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1950.895</td>\n",
       "      <td>0.486179</td>\n",
       "      <td>0.511764</td>\n",
       "      <td>0.511764</td>\n",
       "      <td>0.511764</td>\n",
       "      <td>0.511764</td>\n",
       "      <td>0.513833</td>\n",
       "      <td>0.513833</td>\n",
       "      <td>0.513833</td>\n",
       "      <td>0.513833</td>\n",
       "      <td>0.513833</td>\n",
       "      <td>0.499225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85030</th>\n",
       "      <td>0.440972</td>\n",
       "      <td>2078.12</td>\n",
       "      <td>0.507354</td>\n",
       "      <td>0.492646</td>\n",
       "      <td>0.4464</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2082.095</td>\n",
       "      <td>0.477667</td>\n",
       "      <td>0.512983</td>\n",
       "      <td>0.524703</td>\n",
       "      <td>0.522761</td>\n",
       "      <td>0.519405</td>\n",
       "      <td>0.661465</td>\n",
       "      <td>0.930322</td>\n",
       "      <td>1.088203</td>\n",
       "      <td>1.174482</td>\n",
       "      <td>1.241764</td>\n",
       "      <td>0.495834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85031</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>2087.62</td>\n",
       "      <td>0.511011</td>\n",
       "      <td>0.488989</td>\n",
       "      <td>0.5436</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2078.525</td>\n",
       "      <td>0.477328</td>\n",
       "      <td>0.517299</td>\n",
       "      <td>0.529040</td>\n",
       "      <td>0.527094</td>\n",
       "      <td>0.523732</td>\n",
       "      <td>0.666037</td>\n",
       "      <td>0.935355</td>\n",
       "      <td>1.093508</td>\n",
       "      <td>1.179934</td>\n",
       "      <td>1.247332</td>\n",
       "      <td>0.504294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85032</th>\n",
       "      <td>0.447917</td>\n",
       "      <td>2070.55</td>\n",
       "      <td>0.508169</td>\n",
       "      <td>0.491831</td>\n",
       "      <td>0.6650</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2065.750</td>\n",
       "      <td>0.494978</td>\n",
       "      <td>0.526404</td>\n",
       "      <td>0.539807</td>\n",
       "      <td>0.544130</td>\n",
       "      <td>0.539331</td>\n",
       "      <td>0.682524</td>\n",
       "      <td>0.953508</td>\n",
       "      <td>1.112638</td>\n",
       "      <td>1.199599</td>\n",
       "      <td>1.267414</td>\n",
       "      <td>0.515460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85033</th>\n",
       "      <td>0.451389</td>\n",
       "      <td>2059.36</td>\n",
       "      <td>0.504756</td>\n",
       "      <td>0.495244</td>\n",
       "      <td>0.4322</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2060.670</td>\n",
       "      <td>0.501523</td>\n",
       "      <td>0.532632</td>\n",
       "      <td>0.546068</td>\n",
       "      <td>0.550402</td>\n",
       "      <td>0.545591</td>\n",
       "      <td>0.689137</td>\n",
       "      <td>0.960789</td>\n",
       "      <td>1.120312</td>\n",
       "      <td>1.207487</td>\n",
       "      <td>1.275469</td>\n",
       "      <td>0.506163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85034</th>\n",
       "      <td>0.454861</td>\n",
       "      <td>2057.97</td>\n",
       "      <td>0.509523</td>\n",
       "      <td>0.490477</td>\n",
       "      <td>0.7644</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2051.715</td>\n",
       "      <td>0.518351</td>\n",
       "      <td>0.543686</td>\n",
       "      <td>0.557181</td>\n",
       "      <td>0.561534</td>\n",
       "      <td>0.556702</td>\n",
       "      <td>0.700874</td>\n",
       "      <td>0.973711</td>\n",
       "      <td>1.133931</td>\n",
       "      <td>1.221487</td>\n",
       "      <td>1.289765</td>\n",
       "      <td>0.510912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85035 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open_time     Open      High       Low  Number_of_trades   weekday  \\\n",
       "0       0.927083  1948.28  0.514096  0.485904            0.9999  0.333333   \n",
       "1       0.930556  1966.87  0.506678  0.493322            0.3711  0.333333   \n",
       "2       0.934028  1958.46  0.510884  0.489116            0.6934  0.333333   \n",
       "3       0.937500  1946.10  0.506371  0.493629            0.3920  0.333333   \n",
       "4       0.940972  1948.30  0.503351  0.496649            0.2156  0.333333   \n",
       "...          ...      ...       ...       ...               ...       ...   \n",
       "85030   0.440972  2078.12  0.507354  0.492646            0.4464  0.666667   \n",
       "85031   0.444444  2087.62  0.511011  0.488989            0.5436  0.666667   \n",
       "85032   0.447917  2070.55  0.508169  0.491831            0.6650  0.666667   \n",
       "85033   0.451389  2059.36  0.504756  0.495244            0.4322  0.666667   \n",
       "85034   0.454861  2057.97  0.509523  0.490477            0.7644  0.666667   \n",
       "\n",
       "       mean_price    target  mean_1_hours  mean_2_hours  mean_3_hours  \\\n",
       "0        1958.835  0.483083      0.503644      0.503644      0.503644   \n",
       "1        1963.545  0.471149      0.497638      0.497638      0.497638   \n",
       "2        1953.495  0.486006      0.510488      0.510488      0.510488   \n",
       "3        1950.290  0.484913      0.512543      0.512543      0.512543   \n",
       "4        1950.895  0.486179      0.511764      0.511764      0.511764   \n",
       "...           ...       ...           ...           ...           ...   \n",
       "85030    2082.095  0.477667      0.512983      0.524703      0.522761   \n",
       "85031    2078.525  0.477328      0.517299      0.529040      0.527094   \n",
       "85032    2065.750  0.494978      0.526404      0.539807      0.544130   \n",
       "85033    2060.670  0.501523      0.532632      0.546068      0.550402   \n",
       "85034    2051.715  0.518351      0.543686      0.557181      0.561534   \n",
       "\n",
       "       mean_4_hours  mean_4_days  mean_8_days  mean_12_days  mean_16_days  \\\n",
       "0          0.503644     0.503644     0.503644      0.503644      0.503644   \n",
       "1          0.497638     0.497638     0.497638      0.497638      0.497638   \n",
       "2          0.510488     0.510488     0.510488      0.510488      0.510488   \n",
       "3          0.512543     0.514613     0.514613      0.514613      0.514613   \n",
       "4          0.511764     0.513833     0.513833      0.513833      0.513833   \n",
       "...             ...          ...          ...           ...           ...   \n",
       "85030      0.519405     0.661465     0.930322      1.088203      1.174482   \n",
       "85031      0.523732     0.666037     0.935355      1.093508      1.179934   \n",
       "85032      0.539331     0.682524     0.953508      1.112638      1.199599   \n",
       "85033      0.545591     0.689137     0.960789      1.120312      1.207487   \n",
       "85034      0.556702     0.700874     0.973711      1.133931      1.221487   \n",
       "\n",
       "       mean_20_days  prev_price  \n",
       "0          0.503644    0.491685  \n",
       "1          0.497638    0.494003  \n",
       "2          0.510488    0.512862  \n",
       "3          0.514613    0.504108  \n",
       "4          0.513833    0.499225  \n",
       "...             ...         ...  \n",
       "85030      1.241764    0.495834  \n",
       "85031      1.247332    0.504294  \n",
       "85032      1.267414    0.515460  \n",
       "85033      1.275469    0.506163  \n",
       "85034      1.289765    0.510912  \n",
       "\n",
       "[85035 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd4dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=30\n",
    "#X,y=df_to_X_y(df,window_size) #for no enc\n",
    "X,y=df_to_X_y(df,window_size,0) #no lstm no enc\n",
    "#X,y=tensor_to_X_y(arr,window_size) for enc\n",
    "#del(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad7e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d932e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,drop,hidden_size,test_size):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.norm=nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.Linear(41,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_size, test_size,bias=True)\n",
    "        self.fc4 = nn.Linear(test_size, hidden_size,bias=True)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc7 = nn.Linear(hidden_size, 41,bias=True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "    def encode (self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe0fa60c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoder\u001b[38;5;241m=\u001b[39m\u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m      2\u001b[0m encoder\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(directory\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mencoder_05-07-2022_22-03-48_64_10.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\nn\\modules\\module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\nn\\modules\\module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\nn\\modules\\module.py:749\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    733\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mG:\\anaconda3\\envs\\trading_streamlit\\lib\\site-packages\\torch\\cuda\\__init__.py:221\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "encoder=autoencoder(0,64,10).cuda().double()\n",
    "encoder.load_state_dict(torch.load(directory+'\\\\encoder_05-07-2022_22-03-48_64_10.pth'))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded=encoder.encode(torch.tensor(df.drop([\"Open\",\"target\"],axis=1).to_numpy(),device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr=torch.tensor([0 for i in encoded[:,0]],device=\"cuda\")\n",
    "#count=0\n",
    "#for i in range(encoded.shape[1]):\n",
    "#    if (set(encoded[:,i].tolist())!={0.0}):\n",
    "#        arr=torch.cat((arr,encoded[:,i]),0)\n",
    "#        count+=1\n",
    "#arr=torch.reshape(arr,[count+1,encoded.shape[0]])[1:]\n",
    "#arr=torch.reshape(arr,[encoded.shape[0],count])\n",
    "#arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c914d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr=torch.cat([arr,torch.tensor(df[[\"Open\",\"target\"]].to_numpy(),device=\"cuda\")],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f764c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd764a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.sort(df[\"mean_4_days\"])).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16170cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr=df.corr()\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d94339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[::200],y[::200],\n",
    "                                                        test_size = 0.10,\n",
    "                                                        random_state =123,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "train_idxes=sklearn.utils.shuffle(range(len(X_train)))\n",
    "X_train_shuffled=[]\n",
    "y_train_shuffled=[]\n",
    "for i in train_idxes:\n",
    "    X_train_shuffled.append(X_train[i])\n",
    "    y_train_shuffled.append(y_train[i])\n",
    "X_train=np.array(X_train_shuffled)\n",
    "y_train=np.array(y_train_shuffled)\n",
    "del([X_train_shuffled,y_train_shuffled,train_idxes])\n",
    "\n",
    "xy=1\n",
    "lstm=0\n",
    "\n",
    "\n",
    "if (xy==1 and lstm==0):\n",
    "    X_train_opens=torch.tensor(X_train[:,1],device=\"cuda\") #for df_to_x_y\n",
    "    X_test_opens=torch.tensor(X_test[:,1],device=\"cuda\")\n",
    "    if scaling:\n",
    "        X_train=np.concatenate([X_train[:,2:],X_train[:,[0]]],axis=1)\n",
    "        X_test=np.concatenate([X_test[:,2:],X_test[:,[0]]],axis=1)\n",
    "\n",
    "if (xy==1 and lstm==1):\n",
    "    X_train_opens=torch.tensor(X_train[:,:,1][:,window_size-1],device=\"cuda\") #for df_to_x_y, lstm\n",
    "    X_test_opens=torch.tensor(X_test[:,:,1][:,window_size-1],device=\"cuda\")\n",
    "    if scaling:\n",
    "        X_train=np.concatenate([X_train[:,:,2:],X_train[:,:,[0]]],axis=2)\n",
    "        X_test=np.concatenate([X_test[:,:,2:],X_test[:,:,[0]]],axis=2)\n",
    "\n",
    "if(xy==0):\n",
    "    X_train_opens=torch.tensor(X_train[:,:,-1][:,window_size-1],device=\"cuda\") #for tensor_to_x_y\n",
    "    X_test_opens=torch.tensor(X_test[:,:,-1][:,window_size-1],device=\"cuda\")\n",
    "    if scaling:\n",
    "        X_train=X_train[:,:,:-1]\n",
    "        X_test=X_test[:,:,:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision import transforms\n",
    "X_train_normalized=torch.nn.functional.normalize(torch.tensor(X_train,device=\"cuda\"),2,0)\n",
    "X_test_normalized=torch.nn.functional.normalize(torch.tensor(X_test,device=\"cuda\"),2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b62711",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70430d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELUS\n",
    "\n",
    "class Net_r(nn.Module):\n",
    "    def __init__(self,drop,hidden_size,input_size):\n",
    "        super(Net_r, self).__init__()\n",
    "        self.lstm_active=0\n",
    "        self.norm=nn.BatchNorm1d(hidden_size)\n",
    "        self.lstm = nn.LSTM(num_layers=1,input_size=input_size, hidden_size=hidden_size,batch_first=True)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size,bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc6 = nn.Linear(hidden_size, 1,bias=True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self, x,train):\n",
    "        if (self.lstm_active):\n",
    "            x,_ = self.lstm(x)\n",
    "            x=x[:,-1,:]\n",
    "        else:\n",
    "            x=self.fc1(x)\n",
    "        #x=self.norm(x)\n",
    "        x=F.relu(x)\n",
    "        if train:\n",
    "            #x=self.norm(x)\n",
    "            pass\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=self.fc3(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        if train:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x=self.fc4(x)\n",
    "        x=F.tanh(x)\n",
    "        \n",
    "        x=self.fc5(x)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        if train:\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM##\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,drop,hidden_size,input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm_active=0\n",
    "        self.norm=nn.BatchNorm1d(hidden_size)\n",
    "        self.fc1 = nn.LSTM(num_layers=1,input_size=input_size, hidden_size=hidden_size,batch_first=True)#lstm input\n",
    "        self.fc2 = nn.Linear(input_size, hidden_size,bias=True)#standart input\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size,bias=True)\n",
    "        self.fc6 = nn.Linear(hidden_size, 1,bias=True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "    def forward(self, x,train):\n",
    "        if (self.lstm_active):\n",
    "            x,_ = self.fc1(x)\n",
    "            x=x[:,-1,:]\n",
    "        else:\n",
    "            x=self.fc2(x)\n",
    "        \n",
    "        #x=F.tanh(x)\n",
    "        #x = self.fc3(x)\n",
    "        x=F.tanh(x)\n",
    "        #x = F.logsigmoid(x)\n",
    "        #x=F.relu(x)\n",
    "        if train:\n",
    "            #x=self.norm(x)\n",
    "            #x = self.dropout(x)\n",
    "            pass\n",
    "        #x = F.logsigmoid(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be953dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_func(pred_data,real_data):\n",
    "    mae=0\n",
    "    for i in range(len(pred_data)):\n",
    "        mae+=abs(pred_data[i]-real_data[i])\n",
    "    mae/=len(pred_data)\n",
    "    return mae\n",
    "def train(learning_rate,\n",
    "          batch_size,\n",
    "          epochs,\n",
    "          momentum,train_data,test_data,decay=0,min_error=0,direction_punish=0,direction_reward=0):\n",
    "    \n",
    "    scaling_range=0.2\n",
    "    X_train=train_data[0]\n",
    "    X_test=test_data[0]\n",
    "    y_train=train_data[1]\n",
    "    y_test=test_data[1]\n",
    "    global net,loss_train_single,loss_test_single,scaling\n",
    "    if (train_data[0].shape[0]>2000):\n",
    "        parting=X_train.shape[0]//500\n",
    "        X_test_part=X_test[::parting]\n",
    "        X_train_part=X_train[::parting*10]\n",
    "        y_test_part=y_test[::parting]\n",
    "        y_train_part=y_train[::parting*10]\n",
    "        X_train_part_opens=X_train_opens[::parting*10]\n",
    "        X_test_part_opens=X_test_opens[::parting]\n",
    "    else:\n",
    "        parting=X_train.shape[0]//1#//500\n",
    "        X_test_part=X_test#[::parting]\n",
    "        X_train_part=X_train#[::parting*10]\n",
    "        y_test_part=y_test#[::parting]\n",
    "        y_train_part=y_train#[::parting*10]\n",
    "        X_train_part_opens=X_train_opens\n",
    "        X_test_part_opens=X_test_opens\n",
    "    train_batches=len(X_train)//batch_size\n",
    "    last_batch=len(X_train)%batch_size+1\n",
    "    #net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum,weight_decay=decay)\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,weight_decay=decay)\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    train_loss=nn.L1Loss()\n",
    "    loss_train=[]\n",
    "    loss_test=[]\n",
    "    y_test_part_scaled=torch.tensor(y_test_part,device=\"cuda\")\n",
    "    y_train_part_scaled=torch.tensor(y_train_part,device=\"cuda\")\n",
    "    \n",
    "    if scaling:\n",
    "        y_test_part_scaled=upscale(y_test_part_scaled,scaling_range)*X_test_part_opens\n",
    "        y_train_part_scaled=upscale(y_train_part_scaled,scaling_range)*X_train_part_opens\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(train_batches+1):\n",
    "            if (batch<train_batches):\n",
    "                X_train_batch=torch.tensor(X_train[batch*batch_size:(batch_size*(batch+1))],device=\"cuda\")\n",
    "                X_opens_batch=X_train_opens[batch*batch_size:(batch_size*(batch+1))]\n",
    "                y_train_batch=torch.tensor(y_train[batch*batch_size:(batch_size*(batch+1))],device=\"cuda\")\n",
    "            else:\n",
    "                X_train_batch=torch.tensor(X_train[batch*batch_size:batch*batch_size+last_batch],device=\"cuda\")\n",
    "                X_opens_batch=X_train_opens[batch*batch_size:batch*batch_size+last_batch]\n",
    "                y_train_batch=torch.tensor(y_train[batch*batch_size:batch*batch_size+last_batch],device=\"cuda\")\n",
    "            #y_targets=upscale(y_train_batch,scaling_range)*X_opens_batch\n",
    "            net_out=net(X_train_batch,train=1).reshape(1,-1)[0]#*X_opens_batch\n",
    "            #net_out=net.predict(X_train_batch).reshape(1,-1)[0]#*X_opens_batch\n",
    "            loss = criterion(net_out,y_train_batch)#,y_targets)\n",
    "            #loss.requires_grad=True\n",
    "            #loss = torch.sqrt(criterion(net_out, torch.tensor(y_train_batch,device=\"cuda\")))\n",
    "            if scaling:\n",
    "                for i in range(net_out.shape[0]):\n",
    "                    if((net_out[i]-0.5)*(y_train_batch[i]-0.5)<0):\n",
    "                        loss*=(1+direction_punish)\n",
    "                    else:\n",
    "                        loss*=(1-direction_reward)\n",
    "            #optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #with torch.no_grad():\n",
    "        \n",
    "        #net_out_test=net(torch.tensor(X_test_part,device=\"cuda\"))\n",
    "        #net_out_test=net.predict(torch.tensor(X_test_part,device=\"cuda\")).detach().reshape(1,-1)[0]\n",
    "        #net_out_train=net.predict(torch.tensor(X_train_part,device=\"cuda\")).detach().reshape(1,-1)[0]\n",
    "        net_out_test=net(torch.tensor(X_test_part,device=\"cuda\").detach(),train=0).reshape(1,-1)[0]\n",
    "        net_out_train=net(torch.tensor(X_train_part,device=\"cuda\").detach(),train=0).reshape(1,-1)[0]\n",
    "        if scaling:\n",
    "            net_out_train=upscale(net_out_train,scaling_range)*X_train_part_opens\n",
    "            net_out_test=upscale(net_out_test,scaling_range)*X_test_part_opens\n",
    "            #print(net_out_test[:20])\n",
    "            #mae=mae_func(net_out_test,y_test_part_scaled)\n",
    "            #loss_test.append(train_loss(net_out_test,y_test_part_scaled))#.item())\n",
    "            loss_test_single.append(train_loss(net_out_test,torch.tensor(y_test_part_scaled,device=\"cuda\")).item())#(mae.item())\n",
    "            #net_out_train=net(torch.tensor(X_train_part,device=\"cuda\"))\n",
    "            #loss_train.append(train_loss(net_out_train,y_train_part_scaled))#.item())\n",
    "            loss_train_single.append(train_loss(net_out_train,torch.tensor(y_train_part_scaled,device=\"cuda\")).item())\n",
    "        else:\n",
    "            #loss_test.append(train_loss(net_out_test,y_test_part))#.item())\n",
    "            loss_test_single.append(train_loss(net_out_test,torch.tensor(y_test_part,device=\"cuda\")).item())#(mae.item())\n",
    "            #net_out_train=net(torch.tensor(X_train_part,device=\"cuda\"))\n",
    "            #loss_train.append(train_loss(net_out_train,y_train_part))#.item())\n",
    "            loss_train_single.append(train_loss(net_out_train,torch.tensor(y_train_part,device=\"cuda\")).item())\n",
    "            \n",
    "            gc.collect()\n",
    "        #this_std=np.std(net.predict(torch.tensor(X_test[::70],device=\"cuda\") ).tolist())\n",
    "        #this_std=np.std(net.predict(torch.tensor(X_test,device=\"cuda\") ).tolist())\n",
    "        this_std=np.std(net(torch.tensor(X_test,device=\"cuda\"),train=0 ).tolist())\n",
    "        stds_single.append(this_std)\n",
    "        if (epoch==0):\n",
    "            max_std=0\n",
    "            min_err_test=loss_test_single[-1]\n",
    "            min_err_train=loss_train_single[-1]\n",
    "            \n",
    "            \n",
    "        if (this_std>max_std):\n",
    "            path_std=os.path.join(directory,\"std_net.pth\")\n",
    "            torch.save(net.state_dict(), path_std)\n",
    "            net_max_std.load_state_dict(torch.load(path_std))\n",
    "            max_std=this_std\n",
    "        if (loss_train_single[-1]<min_err_train):\n",
    "            path_min_err_train=os.path.join(directory,\"err_train_net.pth\")\n",
    "            torch.save(net.state_dict(), path_min_err_train)\n",
    "            net_min_err_train.load_state_dict(torch.load(path_min_err_train))\n",
    "            min_err_train=loss_train_single[-1]\n",
    "        if (loss_test_single[-1]<min_err_test):\n",
    "            path_min_err_test=os.path.join(directory,\"err_test_net.pth\")\n",
    "            torch.save(net.state_dict(), path_min_err_test)\n",
    "            net_min_err_test.load_state_dict(torch.load(path_min_err_test))\n",
    "            min_err_test=loss_test_single[-1]\n",
    "        if (epoch%5==1):\n",
    "            #print(\"train_loss: \"+str(loss_train_single[-1])+ \" / test loss: \"+str(loss_test_single[-1])+\" / std: \"+str(this_std))\n",
    "            print('train_loss: %.5f / test loss: %.5f / std: %.5f'% (loss_train_single[-1],loss_test_single[-1], this_std) )\n",
    "        #if (loss_test_single[-1]<min_error):\n",
    "        if (loss_train_single[-1]<min_error):\n",
    "            break\n",
    "        \n",
    "    return loss_train,loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab527d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD NET\n",
    "name=\"\\\\net_24-10-2022_18-33-11_16_128\"\n",
    "path=directory+name\n",
    "net=Net(0.6,128,16).cuda().double()\n",
    "net.load_state_dict(torch.load(path+\".pth\"))\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=128\n",
    "normalized_input=0\n",
    "drop=0#.4\n",
    "net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "runs_count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf06b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_max_std=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "net_min_err_train=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "net_min_err_test=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_single=[]\n",
    "loss_test_single=[]\n",
    "stds_single=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44a3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loss_train_single=[]\n",
    "#loss_test_single=[]\n",
    "lr=0.0001\n",
    "batch_sz=100\n",
    "epochs=1000000\n",
    "decay=0\n",
    "min_err=0.0007\n",
    "momentum=0.8\n",
    "runs_count+=1\n",
    "if normalized_input:\n",
    "    train(learning_rate=lr, batch_size=batch_sz, epochs=epochs, momentum=momentum,decay=decay,train_data=[X_train_normalized,y_train],test_data=[X_test_normalized,y_test],min_error=min_err)#,direction_punish=0.1,direction_reward=0.1)\n",
    "else:\n",
    "    train(learning_rate=lr, batch_size=batch_sz, epochs=epochs, momentum=momentum,decay=decay,train_data=[X_train,y_train],test_data=[X_test,y_test],min_error=min_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e708e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(loss_train_single).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af3a613-7aaf-4f74-a9d7-f49119c08032",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss_train_arr=[]\n",
    "min_loss_test_arr=[]\n",
    "for hidden_size_idx in range(1,20):\n",
    "    hidden_size=hidden_size_idx*64\n",
    "    net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    net_max_std=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    net_min_err_train=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    net_min_err_test=Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "    for batch_sz_idx in range(1,2):\n",
    "        for lr_idx in range (1,4):\n",
    "            train(learning_rate=0.05/(10*lr_idx), batch_size=50-batch_sz_idx*10, epochs=1000, momentum=momentum,decay=decay,train_data=[X_train,y_train],test_data=[X_test,y_test],min_error=min_err)\n",
    "    min_loss_train_arr.append(np.asarray(net_min_err_train(torch.tensor(X_test,device=\"cuda\"),train=0  ).tolist()))\n",
    "    min_loss_test_arr.append(np.asarray(net_min_err_test(torch.tensor(X_test,device=\"cuda\"),train=0  ).tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132bb92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2f6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"train\":loss_train_single[200:],\"test\":loss_test_single[200:]}).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f835fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_test_single[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a98c0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.min(loss_test_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cf6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.std(net(torch.tensor(X_test[::70],device=\"cuda\"),train=0  ).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124be9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stds_single).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c10a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame([loss_train[-0:][33],loss_test[-0:][33]]).T.plot()\n",
    "#preds=pd.DataFrame(net.predict(torch.tensor(X_test[::70][:-15],device=\"cuda\") ).tolist())\n",
    "#array=np.asarray(net_max_std.predict(torch.tensor(X_test[::70][:-15],device=\"cuda\") ).tolist())\n",
    "#for i in range(len(array[0])):\n",
    "    #preds.append(np.mean(array[:,i]))\n",
    "#preds=pd.DataFrame(preds)  \n",
    "if normalized_input:\n",
    "    preds=pd.DataFrame(net(torch.tensor(X_test_normalized,device=\"cuda\"),train=0 ).tolist())\n",
    "    array=np.asarray(net_max_std(torch.tensor(X_test_normalized,device=\"cuda\"),train=0  ).tolist())\n",
    "    min_train=np.asarray(net_min_err_train(torch.tensor(X_train_normalized,device=\"cuda\"),train=0  ).tolist())\n",
    "else:\n",
    "    preds=pd.DataFrame(net(torch.tensor(X_test,device=\"cuda\"),train=0 ).tolist())\n",
    "    array=np.asarray(net_max_std(torch.tensor(X_test,device=\"cuda\"),train=0  ).tolist())\n",
    "    min_train=np.asarray(net_min_err_train(torch.tensor(X_test,device=\"cuda\"),train=0  ).tolist())\n",
    "\n",
    "#data=pd.DataFrame(y_test[::70][:-15])\n",
    "data=pd.DataFrame({\"targets\":y_test})\n",
    "data[\"preds\"]=preds\n",
    "data[\"max_std_preds\"]=array\n",
    "data[\"min_train\"]=min_train\n",
    "#data[\"0.5\"]=pd.DataFrame([0.5 for i in range(preds.size)])\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([loss_train[-0:][33],loss_test[-0:][33]]).T.plot()\n",
    "#preds=pd.DataFrame(net.predict(torch.tensor(X_test[::70][:-15],device=\"cuda\") ).tolist())\n",
    "#array=np.asarray(net_max_std.predict(torch.tensor(X_test[::70][:-15],device=\"cuda\") ).tolist())\n",
    "#for i in range(len(array[0])):\n",
    "    #preds.append(np.mean(array[:,i]))\n",
    "#preds=pd.DataFrame(preds) \n",
    "if X_train.shape[0]>1000:\n",
    "    if normalized_input:\n",
    "        preds=pd.DataFrame(net(torch.tensor(X_train_normalized[::100][-15:],device=\"cuda\"),train=0  ).tolist())\n",
    "        array=np.asarray(net_max_std(torch.tensor(X_train_normalized[::100][-15:],device=\"cuda\"),train=0  ).tolist())\n",
    "        min_train=np.asarray(net_min_err_train(torch.tensor(X_train_normalized[::100][-15:],device=\"cuda\"),train=0  ).tolist())\n",
    "    else:\n",
    "        preds=pd.DataFrame(net(torch.tensor(X_train[::100][-15:],device=\"cuda\"),train=0  ).tolist())\n",
    "        array=np.asarray(net_max_std(torch.tensor(X_train[::100][-15:],device=\"cuda\"),train=0  ).tolist())\n",
    "        min_train=np.asarray(net_min_err_train(torch.tensor(X_train[::100][-15:],device=\"cuda\"),train=0  ).tolist())\n",
    "    data=pd.DataFrame({\"targets\":y_train[::100][-15:]})\n",
    "else:\n",
    "    if normalized_input:\n",
    "        preds=pd.DataFrame(net(torch.tensor(X_train_normalized,device=\"cuda\"),train=0  ).tolist())\n",
    "        array=np.asarray(net_max_std(torch.tensor(X_train_normalized,device=\"cuda\"),train=0  ).tolist())\n",
    "        min_train=np.asarray(net_min_err_train(torch.tensor(X_train_normalized,device=\"cuda\"),train=0  ).tolist())\n",
    "    else:\n",
    "        preds=pd.DataFrame(net(torch.tensor(X_train,device=\"cuda\"),train=0  ).tolist())\n",
    "        array=np.asarray(net_max_std(torch.tensor(X_train,device=\"cuda\"),train=0  ).tolist())\n",
    "        min_train=np.asarray(net_min_err_train(torch.tensor(X_train,device=\"cuda\"),train=0  ).tolist())\n",
    "    data=pd.DataFrame({\"targets\":y_train})\n",
    "\n",
    "#data=pd.DataFrame(y_test[::70][:-15])\n",
    "\n",
    "#data[\"preds\"]=preds\n",
    "data[\"min_train\"]=min_train\n",
    "#data[\"max_std_preds\"]=array\n",
    "#data[\"0.5\"]=pd.DataFrame([0.5 for i in range(preds.size)])\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ba539",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f268ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE NET\n",
    "time_now=datetime.datetime.now()\n",
    "time_now=time_now.strftime(\"\\\\net_%d-%m-%Y_%H-%M-%S_\")+str(X_test.shape[2])+\"_\"+str(hidden_size)\n",
    "path=directory\n",
    "end=\".pth\"\n",
    "path=path+time_now+end\n",
    "torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92814743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH NET PARAMETERS\n",
    "loss_test=[]\n",
    "loss_train=[]\n",
    "stds=[]\n",
    "epoch=20\n",
    "b_size=50\n",
    "drop=0.6\n",
    "decay=0\n",
    "lr=0.002\n",
    "iter_count=0\n",
    "hidden_size=128\n",
    "min_params_test=[]\n",
    "min_params_train=[]\n",
    "min_loss=[10000,1000]\n",
    "#for epoch in range(10,20,5):\n",
    "for lr in np.arange(0.001,0.011,0.001):\n",
    "    #for drop in np.arange(0.55,0.76,0.05):\n",
    "    print(lr)\n",
    "        #for b_size in range(25,55,25):\n",
    "    #for hidden_size in range(64,129,64):\n",
    "    for decay in np.arange(0,0.013,0.002):\n",
    "        net = Net(drop,hidden_size,X_train.shape[-1]).to(device).double()\n",
    "        loss=train(lr, b_size, epoch,0.9,decay=decay,direction_punish=0.01,direction_reward=0.01)\n",
    "        #drop,learning_rate,batch_size,epochs,momentum,hidden_size=128,decay=0\n",
    "        for i in range(epoch):\n",
    "            # 0 - train, 1 - test\n",
    "            if loss[0][i]<min_loss[0]:\n",
    "                min_loss[0]=loss[0][i]\n",
    "                min_params_train=[lr,drop,b_size,hidden_size,decay,i,iter_count]\n",
    "            if loss[1][i]<min_loss[1]:\n",
    "                min_loss[1]=loss[1][i]\n",
    "                min_params_test=[lr,drop,b_size,hidden_size,decay,i,iter_count]\n",
    "        iter_count+=1\n",
    "        loss_test.append(loss[1])\n",
    "        loss_train.append(loss[0])\n",
    "        stds.append(np.std(net.predict(torch.tensor(X_test[:15],device=\"cuda\") ).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be120212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decay,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ca16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_mean=[]\n",
    "loss_test_mean=[]\n",
    "for i in range(len(loss_test)):\n",
    "    loss_train_mean.append(np.mean(loss_train[i][2:]))\n",
    "    loss_test_mean.append(np.mean(loss_test[i][2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20286778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([loss_train_mean[:50],loss_test_mean[:50]]).T.plot()\n",
    "pd.DataFrame([loss_train_mean,loss_test_mean]).T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbf758",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stds).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a00cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49180f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_params_test \n",
    "#lr, drop, b_size, h_size, decay, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(net.fc5.weight.tolist()).max().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460424b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(loss_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa33b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(loss_train_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb980c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717bfdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_train=[]\n",
    "#loss_test=[]\n",
    "#loss_train_mean=[]\n",
    "#loss_test_mean=[]\n",
    "#for i in range(7):\n",
    "#    window_size=15*(i+1)\n",
    "#    X,y=df_to_X_y(df,window_size)\n",
    "#    X_train, X_test, y_train, y_test = train_test_split(X[:],\n",
    "#                                                        y[:],\n",
    "#                                                        test_size = 0.10,\n",
    "#                                                        random_state =123,\n",
    "#                                                        shuffle=False)\n",
    "#    parting=X_test.shape[0]//100\n",
    "#    X_train_part=X_train[::parting]\n",
    "#    X_test_part=X_test[::parting]\n",
    "#    y_train_part=y_train[::parting]\n",
    "#    y_test_part=y_test[::parting]\n",
    "#    loss=train(drop=0.2, learning_rate=0.004, batch_size=100, epochs=20, momentum=0.9,hidden_size=64,decay=0.00000001)\n",
    "#    loss_train_mean.append(np.mean(loss[0]))\n",
    "#    loss_test_mean.append(np.mean(loss[1]))\n",
    "#    loss_train.append(loss[0])\n",
    "#    loss_test.append(loss[1])\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6189f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f962cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=loss[2]\n",
    "net(torch.tensor(X_test[:10],device=\"cuda\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ec598",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d906ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(net.fc2.weight.max(),\n",
    "net.fc3.weight.max(),\n",
    "net.fc4.weight.max(),\n",
    "net.fc5.weight.max(),\n",
    "net.fc6.weight.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.fc2.bias,\n",
    "net.fc3.bias,\n",
    "net.fc4.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052bb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c41c5-318d-4ea3-ada4-1912eff9e781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
